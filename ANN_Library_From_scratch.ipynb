{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JzuZLNbya6Fg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The neuron"
      ],
      "metadata": {
        "id": "gtEA4NrRbTWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron(object):\n",
        "      \"\"\"\n",
        "    A simple artificial neuron, processing an input vector and returning a corresponding activation.\n",
        "    Args:\n",
        "        num_inputs (int): The input vector size / number of input values.\n",
        "        activation_function (callable): The activation function defining this neuron.\n",
        "    Attributes:\n",
        "        W (ndarray): The weight values for each input.\n",
        "        b (float): The bias value, added to the weighted sum.\n",
        "        activation_function (callable): The activation function computing the neuron's output.\n",
        "    \"\"\"\n",
        "      def __init__(self, num_inputs, activation_function):\n",
        "        super().__init__()\n",
        "        self.W = np.random.uniform(size = num_inputs, low = -1, high = 1)\n",
        "        self.b = np.random.uniform(size = 1, low = -1, high = 1)\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "      def forward(self,x):\n",
        "        \"\"\"\n",
        "        Forward the input signal through the neuron, returning its activation value.\n",
        "        Args:\n",
        "            x (ndarray): The input vector, of shape `(1, num_inputs)`\n",
        "        Returns:\n",
        "            activation (ndarray): The activation value, of shape `(1, layer_size)`.\n",
        "        \"\"\"\n",
        "        z = np.dot(x, self.W) + self.b\n",
        "        return self.activation_function(z)\n",
        "\n"
      ],
      "metadata": {
        "id": "e6xVEzW3bS2m"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class represents a simple artificial neuron. This will receive a vector of input values to merge and process them before an activation value. First we instantiate our neuron. Let us create a perceptron taking 2 input values and using the step function for computing its activation. Its weights and bias values are randomly set"
      ],
      "metadata": {
        "id": "ArwGh2KIbywq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 3\n",
        "step_function = lambda y: 0 if y <= 0 else 1\n",
        "perceptron = Neuron(num_inputs = input_size, activation_function=step_function)\n",
        "print(\"Perceptron's random weights = {} and random bias = {}\".format(perceptron.W, perceptron.b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wsf_ldD1bPkV",
        "outputId": "19c6855a-d4ca-4097-88ba-8983a917825f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perceptron's random weights = [-0.60065244  0.02846888  0.18482914] and random bias = [-0.90709917]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.rand(input_size).reshape(1,input_size)\n",
        "print(\"Input vector = {}\". format(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOvkGWrWfLYf",
        "outputId": "6dc3b3e6-333f-488b-cb14-fbff98791b47"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input vector = [[0.60754485 0.17052412 0.06505159]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = perceptron.forward(x)\n",
        "print(\"Perceptron's output value given x:{}\".format(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFWz8hUQfaUE",
        "outputId": "bf0f3c54-224b-446e-8507-d66da3644339"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perceptron's output value given x:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layering Neurons Together"
      ],
      "metadata": {
        "id": "2cHefnHpgKNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FullyConnectedLayer(object):\n",
        "    \"\"\"A simple fully-connected NN layer.\n",
        "    Args:\n",
        "        num_inputs (int): The input vector size / number of input values.\n",
        "        layer_size (int): The output vector size / number of neurons in the layer.\n",
        "        activation_function (callable): The activation function for this layer.\n",
        "    Attributes:\n",
        "        W (ndarray): The weight values for each input.\n",
        "        b (ndarray): The bias value, added to the weighted sum.\n",
        "        size (int): The layer size / number of neurons.\n",
        "        activation_function (callable): The activation function computing the neuron's output.\n",
        "        x (ndarray): The last provided input vector, stored for backpropagation.\n",
        "        y (ndarray): The corresponding output, also stored for backpropagation.\n",
        "        derivated_activation_function (callable): The corresponding derivated function for backpropagation.\n",
        "        dL_dW (ndarray): The derivative of the loss, with respect to the weights W.\n",
        "        dL_db (ndarray): The derivative of the loss, with respect to the bias b.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_inputs, layer_size, activation_function, derivated_activation_function=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # Randomly initializing the weight vector and the bias value (using a normal distribution this time):\n",
        "        self.W = np.random.standard_normal((num_inputs, layer_size))\n",
        "        self.b = np.random.standard_normal(layer_size)\n",
        "        self.size = layer_size\n",
        "\n",
        "        self.activation_function = activation_function\n",
        "        self.derivated_activation_function = derivated_activation_function\n",
        "        self.x, self.y = None, None\n",
        "        self.dL_dW, self.dL_db = None, None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward the input vector through the layer, returning its activation vector.\n",
        "        Args:\n",
        "            x (ndarray): The input vector, of shape `(batch_size, num_inputs)`\n",
        "        Returns:\n",
        "            activation (ndarray): The activation value, of shape `(batch_size, layer_size)`.\n",
        "        \"\"\"\n",
        "        z = np.dot(x, self.W) + self.b\n",
        "        self.y = self.activation_function(z)\n",
        "        self.x = x  # (we store the input and output values for back-propagation)\n",
        "        return self.y\n",
        "\n",
        "    def backward(self, dL_dy):\n",
        "        \"\"\"\n",
        "        Back-propagate the loss, computing all the derivatives, storing those w.r.t. the layer parameters,\n",
        "        and returning the loss w.r.t. its inputs for further propagation.\n",
        "        Args:\n",
        "            dL_dy (ndarray): The loss derivative w.r.t. the layer's output (dL/dy = l'_{k+1}).\n",
        "        Returns:\n",
        "            dL_dx (ndarray): The loss derivative w.r.t. the layer's input (dL/dx).\n",
        "        \"\"\"\n",
        "        dy_dz = self.derivated_activation_function(self.y)  # = f'\n",
        "        dL_dz = (dL_dy * dy_dz) # dL/dz = dL/dy * dy/dz = l'_{k+1} * f'\n",
        "        dz_dw = self.x.T\n",
        "        dz_dx = self.W.T\n",
        "        dz_db = np.ones(dL_dy.shape[0]) # dz/db = d(W.x + b)/db = 0 + db/db = \"ones\"-vector\n",
        "\n",
        "        # Computing the derivatives with respect to the layer's parameters, and storing them for opt. optimization:\n",
        "        self.dL_dW = np.dot(dz_dw, dL_dz)\n",
        "        self.dL_db = np.dot(dz_db, dL_dz)\n",
        "\n",
        "        # Computing the derivative with respect to the input, to be passed to the previous layers (their `dL_dy`):\n",
        "        dL_dx = np.dot(dL_dz, dz_dx)\n",
        "        return dL_dx\n",
        "\n",
        "    def optimize(self, epsilon):\n",
        "        \"\"\"\n",
        "        Optimize the layer's parameters, using the stored derivative values.\n",
        "        Args:\n",
        "            epsilon (float): The learning rate.\n",
        "        \"\"\"\n",
        "        self.W -= epsilon * self.dL_dW\n",
        "        self.b -= epsilon * self.dL_db"
      ],
      "metadata": {
        "id": "ZoFDLCiAfwXt"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 2\n",
        "num_neurons  = 3\n",
        "relu_function = lambda y: np.maximum(y,0)\n",
        "layer = FullyConnectedLayer(num_inputs = input_size, layer_size = num_neurons, activation_function = relu_function)"
      ],
      "metadata": {
        "id": "GbBok7dsf1dc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = np.random.uniform(-1,1,2).reshape(1,2)\n",
        "print(\"Input vector #1: {}\".format(x1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rbOy-0Lf1tM",
        "outputId": "b5c21cce-df5e-4005-bd46-d9e31047d885"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input vector #1: [[ 0.08539217 -0.71815155]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = np.random.uniform(-1,1,2).reshape(1,2)\n",
        "print(\"Input vector #2 : {}\".format(x2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8yihyKWjyvw",
        "outputId": "9a27e14a-ec03-4045-f0ac-e0b9237fa4ce"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input vector #2 : [[ 0.60439396 -0.85089871]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y1 = layer.forward(x1)\n",
        "print(\"Layer's output value given `x1` : {}\".format(y1))\n",
        "y2 = layer.forward(x2)\n",
        "print(\"Layer's output value given `x2` : {}\".format(y2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDk5mIslj4T5",
        "outputId": "df37e55e-ab67-4cca-a878-0009f64947e6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer's output value given `x1` : [[0.21978189 0.         0.        ]]\n",
            "Layer's output value given `x2` : [[0.55580789 0.13268859 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x12 = np.concatenate((x1, x2))  # stack of input vectors, of shape `(2, 2)`\n",
        "y12 = layer.forward(x12)\n",
        "print(\"Layer's output value given `[x1, x2]` :\\n{}\".format(y12))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdxuiKCXkF_j",
        "outputId": "0864b4d7-73f4-416f-9af2-8ea29fc15c45"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer's output value given `[x1, x2]` :\n",
            "[[0.21978189 0.         0.        ]\n",
            " [0.55580789 0.13268859 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing a complete neural network"
      ],
      "metadata": {
        "id": "N972W0Oqkl2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):             # sigmoid function\n",
        "    y = 1 / (1 + np.exp(-x))\n",
        "    return y\n",
        "\n",
        "\n",
        "def derivated_sigmoid(y):   # sigmoid derivative function\n",
        "    return y * (1 - y)"
      ],
      "metadata": {
        "id": "jtImWib5kQjE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_L2(pred, target):             # L2 loss function\n",
        "    return np.sum(np.square(pred - target)) / pred.shape[0] # opt. we divide by the batch size\n",
        "\n",
        "\n",
        "def derivated_loss_L2(pred, target):   # L2 derivative function\n",
        "    return 2 * (pred - target)"
      ],
      "metadata": {
        "id": "vHEwb7vYkqJO"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(pred, target):            # cross-entropy loss function\n",
        "    return -np.mean(np.multiply(np.log(pred), target) + np.multiply(np.log(1 - pred), (1 - target)))\n",
        "\n",
        "\n",
        "def derivated_binary_cross_entropy(pred, target):  # cross-entropy derivative function\n",
        "    return (pred - target) / (pred * (1 - pred))"
      ],
      "metadata": {
        "id": "x4OdGXPnkrXA"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNetwork(object):\n",
        "    \"\"\"A simple fully-connected NN.\n",
        "    Args:\n",
        "        num_inputs (int): The input vector size / number of input values.\n",
        "        num_outputs (int): The output vector size.\n",
        "        hidden_layers_sizes (list): A list of sizes for each hidden layer to add to the network\n",
        "        activation_function (callable): The activation function for all the layers\n",
        "        derivated_activation_function (callable): The derivated activation function\n",
        "        loss_function (callable): The loss function to train this network\n",
        "        derivated_loss_function (callable): The derivative of the loss function, for back-propagation\n",
        "    Attributes:\n",
        "        layers (list): The list of layers forming this simple network.\n",
        "        loss_function (callable): The loss function to train this network.\n",
        "        derivated_loss_function (callable): The derivative of the loss function, for back-propagation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_inputs, num_outputs, hidden_layers_sizes=(64, 32),\n",
        "                 activation_function=sigmoid, derivated_activation_function=derivated_sigmoid,\n",
        "                 loss_function=loss_L2, derivated_loss_function=derivated_loss_L2):\n",
        "        super().__init__()\n",
        "        # We build the list of layers composing the network, according to the provided arguments:\n",
        "        layer_sizes = [num_inputs, *hidden_layers_sizes, num_outputs]\n",
        "        self.layers = [\n",
        "            FullyConnectedLayer(layer_sizes[i], layer_sizes[i + 1],\n",
        "                                activation_function, derivated_activation_function)\n",
        "            for i in range(len(layer_sizes) - 1)]\n",
        "\n",
        "        self.loss_function = loss_function\n",
        "        self.derivated_loss_function = derivated_loss_function\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward the input vector through the layers, returning the output vector.\n",
        "        Args:\n",
        "            x (ndarray): The input vector, of shape `(batch_size, num_inputs)`.\n",
        "        Returns:\n",
        "            activation (ndarray): The output activation value, of shape `(batch_size, layer_size)`.\n",
        "        \"\"\"\n",
        "        for layer in self.layers: # from the input layer to the output one\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Compute the output corresponding to input `x`, and return the index of the largest\n",
        "        output value.\n",
        "        Args:\n",
        "            x (ndarray): The input vector, of shape `(1, num_inputs)`.\n",
        "        Returns:\n",
        "            best_class (int): The predicted class ID.\n",
        "        \"\"\"\n",
        "        estimations = self.forward(x)\n",
        "        best_class = np.argmax(estimations)\n",
        "        return best_class\n",
        "\n",
        "    def backward(self, dL_dy):\n",
        "        \"\"\"\n",
        "        Back-propagate the loss hrough the layers (require `forward()` to be called before).\n",
        "        Args:\n",
        "            dL_dy (ndarray): The loss derivative w.r.t. the network's output (dL/dy).\n",
        "        Returns:\n",
        "            dL_dx (ndarray): The loss derivative w.r.t. the network's input (dL/dx).\n",
        "        \"\"\"\n",
        "        for layer in reversed(self.layers): # from the output layer to the input one\n",
        "            dL_dy = layer.backward(dL_dy)\n",
        "        return dL_dy\n",
        "\n",
        "    def optimize(self, epsilon):\n",
        "        \"\"\"\n",
        "        Optimize the network parameters according to the stored gradients (require `backward()`\n",
        "        to be called before).\n",
        "        Args:\n",
        "            epsilon (float): The learning rate.\n",
        "        \"\"\"\n",
        "        for layer in self.layers:             # the order doesn't matter here\n",
        "            layer.optimize(epsilon)\n",
        "\n",
        "    def evaluate_accuracy(self, X_val, y_val):\n",
        "        \"\"\"\n",
        "        Given a dataset and its ground-truth labels, evaluate the current accuracy of the network.\n",
        "        Args:\n",
        "            X_val (ndarray): The input validation dataset.\n",
        "            y_val (ndarray): The corresponding ground-truth validation dataset.\n",
        "        Returns:\n",
        "            accuracy (float): The accuracy of the network\n",
        "                              (= number of correct predictions/dataset size).\n",
        "        \"\"\"\n",
        "        num_corrects = 0\n",
        "        for i in range(len(X_val)):\n",
        "            pred_class = self.predict(X_val[i])\n",
        "            if pred_class == y_val[i]:\n",
        "                num_corrects += 1\n",
        "        return num_corrects / len(X_val)\n",
        "\n",
        "    def train(self, X_train, y_train, X_val=None, y_val=None,\n",
        "              batch_size=32, num_epochs=5, learning_rate=1e-3, print_frequency=20):\n",
        "        \"\"\"\n",
        "        Given a dataset and its ground-truth labels, evaluate the current accuracy of the network.\n",
        "        Args:\n",
        "            X_train (ndarray): The input training dataset.\n",
        "            y_train (ndarray): The corresponding ground-truth training dataset.\n",
        "            X_val (ndarray): The input validation dataset.\n",
        "            y_val (ndarray): The corresponding ground-truth validation dataset.\n",
        "            batch_size (int): The mini-batch size.\n",
        "            num_epochs (int): The number of training epochs i.e. iterations over the whole dataset.\n",
        "            learning_rate (float): The learning rate to scale the derivatives.\n",
        "            print_frequency (int): Frequency to print metrics (in epochs).\n",
        "        Returns:\n",
        "            losses (list): The list of training losses for each epoch.\n",
        "            accuracies (list): The list of validation accuracy values for each epoch.\n",
        "        \"\"\"\n",
        "        num_batches_per_epoch = len(X_train) // batch_size\n",
        "        do_validation = X_val is not None and y_val is not None\n",
        "        losses, accuracies = [], []\n",
        "        for i in range(num_epochs): # for each training epoch\n",
        "            epoch_loss = 0\n",
        "            for b in range(num_batches_per_epoch):  # for each batch composing the dataset\n",
        "                # Get batch:\n",
        "                batch_index_begin = b * batch_size\n",
        "                batch_index_end = batch_index_begin + batch_size\n",
        "                x = X_train[batch_index_begin: batch_index_end]\n",
        "                targets = y_train[batch_index_begin: batch_index_end]\n",
        "                # Optimize on batch:\n",
        "                predictions = y = self.forward(x)  # forward pass\n",
        "                L = self.loss_function(predictions, targets)  # loss computation\n",
        "                dL_dy = self.derivated_loss_function(predictions, targets)  # loss derivation\n",
        "                self.backward(dL_dy)  # back-propagation pass\n",
        "                self.optimize(learning_rate)  # optimization of the NN\n",
        "                epoch_loss += L\n",
        "\n",
        "            # Logging training loss and validation accuracy, to follow the training:\n",
        "            epoch_loss /= num_batches_per_epoch\n",
        "            losses.append(epoch_loss)\n",
        "            if do_validation:\n",
        "                accuracy = self.evaluate_accuracy(X_val, y_val)\n",
        "                accuracies.append(accuracy)\n",
        "            else:\n",
        "                accuracy = np.NaN\n",
        "            if i % print_frequency == 0 or i == (num_epochs - 1):\n",
        "                print(\"Epoch {:4d}: training loss = {:.6f} | val accuracy = {:.2f}%\".format(\n",
        "                    i, epoch_loss, accuracy * 100))\n",
        "        return losses, accuracies"
      ],
      "metadata": {
        "id": "N2P2i4Dwk1BQ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "# !pip install matplotlib  # Uncomment and run if matplotlib is not installed yet.\n",
        "import matplotlib          # We use this package to visualize some data and results\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "L5_SFHmslzpm"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "num_classes = 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRyrT2CKl1YO",
        "outputId": "c232f8c4-e677-45e1-cae6-8c7a7f05c41b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_idx = np.random.randint(0, x_test.shape[0])\n",
        "plt.imshow(x_test[img_idx], cmap=matplotlib.cm.binary)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "emidwvrUmE4n",
        "outputId": "64694f88-7083-495d-b85e-7de43b52b321"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[img_idx]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1Y9i4HTmGXQ",
        "outputId": "297ed773-9353-4fed-8cae-95819cefa506"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test = x_train.reshape(-1, 28 * 28), x_test.reshape(-1, 28 * 28)"
      ],
      "metadata": {
        "id": "H09relHYmJWx"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Pixel values between {} and {}\".format(X_train.min(), X_train.max()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOYzNDgPmLeU",
        "outputId": "3271e8fc-b4a0-49ce-9ef8-7a33448dcf8e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pixel values between 0 and 255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test = X_train / 255., X_test / 255.\n",
        "print(\"Normalized pixel values between {} and {}\".format(X_train.min(), X_train.max()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqu7dcu0mMFZ",
        "outputId": "d49e6b87-c042-40f2-8e5c-25cb7c854c1f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized pixel values between 0.0 and 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.eye(num_classes)[y_train]"
      ],
      "metadata": {
        "id": "GH71Sx58mQCy"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_classifier = SimpleNetwork(num_inputs=X_train.shape[1],\n",
        "                                 num_outputs=num_classes, hidden_layers_sizes=[64, 32])"
      ],
      "metadata": {
        "id": "iJTEwmlHmRKL"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = mnist_classifier.forward(X_train)                         # forward pass\n",
        "loss_untrained = mnist_classifier.loss_function(predictions, y_train)   # loss computation\n",
        "\n",
        "accuracy_untrained = mnist_classifier.evaluate_accuracy(X_test, y_test)  # Accuracy\n",
        "print(\"Untrained : training loss = {:.6f} | val accuracy = {:.2f}%\".format(\n",
        "    loss_untrained, accuracy_untrained * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBGYD84FmSSp",
        "outputId": "c4f122b4-6388-46c6-bf54-205c8398ce3b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Untrained : training loss = 4.436700 | val accuracy = 12.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses, accuracies = mnist_classifier.train(X_train, y_train, X_test, y_test,\n",
        "                                            batch_size=30, num_epochs=60)\n",
        "# note: Reduce the batch size and/or number of epochs if your computer can't\n",
        "#       handle the computations / takes too long.\n",
        "#       Remember, numpy also uses the CPU, not GPUs as modern Deep Learning\n",
        "#       libraries do, hence the lack of computational performance here."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbXqtGnPmTWk",
        "outputId": "781d620a-3545-4a25-8981-cc9ece0d4717"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0: training loss = 0.142101 | val accuracy = 90.74%\n",
            "Epoch   20: training loss = 0.125006 | val accuracy = 91.46%\n",
            "Epoch   40: training loss = 0.112432 | val accuracy = 92.13%\n",
            "Epoch   59: training loss = 0.103216 | val accuracy = 92.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses, accuracies = [loss_untrained] + losses, [accuracy_untrained] + accuracies\n",
        "fig, ax_loss = plt.subplots()\n",
        "\n",
        "color = 'red'\n",
        "ax_loss.set_xlim([0, 60])\n",
        "ax_loss.set_xlabel('Epochs')\n",
        "ax_loss.set_ylabel('Training Loss', color=color)\n",
        "ax_loss.plot(losses, color=color)\n",
        "ax_loss.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "ax_acc = ax_loss.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "color = 'blue'\n",
        "ax_acc.set_xlim([0, 65])\n",
        "ax_acc.set_ylim([0, 1])\n",
        "ax_acc.set_ylabel('Val Accuracy', color=color)\n",
        "ax_acc.plot(accuracies, color=color)\n",
        "ax_acc.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "58k6yrWImVMK",
        "outputId": "1d5556d3-2974-47f4-a915-58fc696f398d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We use `np.expand_dims(x, 0)` to simulate a batch (transforming the image shape\n",
        "# from (784,) to (1, 784)):\n",
        "predicted_class = mnist_classifier.predict(np.expand_dims(X_test[img_idx], 0))\n",
        "print('Predicted class: {}; Correct class: {}'.format(predicted_class, y_test[img_idx]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wq0aEMeWnjss",
        "outputId": "e5b7ae50-1535-4869-a166-496dc78f4249"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 1; Correct class: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s50vPHjfnuGE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}