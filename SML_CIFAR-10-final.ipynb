{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Deep Neural Network Method with Convolution**\n\nThe classification deep learning model has been created with convolution layers. The data has been split into training and test data. The convolution layers are inserted using the Conv2D function with ReLU as the activation function. Max Pooling has also been done to adjust the size of the image. Dropout has also been implemented to prevent overfitting and regularize the model. The architectureuses the adam optimizer algorithm. The metrics for evaluation of each epoch is accuracy. There are a total of 2 hidden layers and the output layer consists of a softmax classifier. The change in accuracy and the loss are visualized for gaining insights.","metadata":{}},{"cell_type":"code","source":"#importing the required libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom keras.utils import to_categorical\nfrom keras.datasets import cifar10\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nfrom keras import datasets\nfrom keras.utils import np_utils\nimport seaborn as sns\nfrom keras import layers, models\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport warnings\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n# Disable all warnings\nimport time\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load CIFAR-10 dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\nfig, axs = plt.subplots(1,2,figsize=(15,5)) \n# Count plot for training set\nsns.countplot(x = y_train.ravel(), ax=axs[0])\naxs[0].set_title('Class Distribution of training data')\naxs[0].set_xlabel('Classes')\n# Count plot for testing set\nsns.countplot(x = y_test.ravel(), ax=axs[1])\naxs[1].set_title('Class Distribution of Testing data')\naxs[1].set_xlabel('Classes')\nplt.show()\n# Convert class labels to one-hot encoded vectors\nnum_classes = 10\ny_train = to_categorical(y_train, num_classes)\ny_test = to_categorical(y_test, num_classes)\n\n# Normalize pixel values to be between 0 and 1\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\nClasses = ['Airplane', 'Automobile','Bird','Cat','Deer','Dog','Frog','Horse','Ship','Truck']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,10,figsize=(20,10))\nfor i in range(0,10):\n    axes[i].imshow(x_train[i])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define model architecture\nmodel = Sequential()\n#Adding Convolution layer with relu activation\nmodel.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=x_train.shape[1:]))\nmodel.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\n#Adding a softmax classifier layer for multicalss classification\nmodel.add(Dense(num_classes, activation='softmax'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile the model\nstart_time = time.time()\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), verbose=1)\nend_time = time.time()\n# Evaluate the model\nscores = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])\nprint('Convergence time:', end_time - start_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training loss and accuracy\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'val'], loc='upper right')\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train', 'val'], loc='lower right')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on test data\ny_pred = model.predict(x_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_test_classes = np.argmax(y_test, axis=1)\n# Calculate accuracy on test data\naccuracy = accuracy_score(y_test_classes, y_pred_classes)\nprint('Test accuracy:', accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test_classes, y_pred_classes)\n\n# Create a heatmap of the confusion matrix\nsns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\",xticklabels = Classes, yticklabels = Classes)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = accuracy_score(y_test_classes, y_pred_classes)\nprecision = precision_score(y_test_classes, y_pred_classes, average='macro')\nrecall = recall_score(y_test_classes, y_pred_classes, average='macro')\nf1 = f1_score(y_test_classes, y_pred_classes, average='macro')\n\n# Print results\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-Score:\", f1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test_classes, y_pred_classes))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep Neural Network without convolution\n\nThe classification deep learning model has been creataed without convolution layers. The data has been split into training and test data. The shape of the images are found out using the shape parameter. The neural network architecture is then created with ReLU function as the activation function with the adam optimizer algorithm. The metrics for evaluation of each epoch is accuracy. There are a total of 2 hidden layers and the output layer consists of a softmax classifier. The change in accuracy and the loss are visualized for gaining insights.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom keras.utils import np_utils\nfrom keras.datasets import cifar10\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Dropout\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# Loading the dataset and splitting it into train and test sets\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n\n# Converting labels to one-hot encoded vectors\ny_train = np_utils.to_categorical(y_train)\ny_test = np_utils.to_categorical(y_test)\n\n# Reshaping the input data\nL, W, H, C = X_train.shape\nX_train = X_train.reshape(-1, W*H*C)\nX_test = X_test.reshape(-1, W*H*C)\n\n# Normalizing the input data\nX_train = X_train / 255.0\nX_test = X_test / 255.0 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the model\nmodel = Sequential()\nmodel.add(Dense(100, input_shape=X_train[1].shape, activation='relu', name=\"Hidden-1\"))\nmodel.add(BatchNormalization())  # Add batch normalization layer\nmodel.add(Dropout(0))\nmodel.add(Dense(50, activation='relu',name='Hidden-2'))\nmodel.add(BatchNormalization())  # Add batch normalization layer\nmodel.add(Dropout(0))\nmodel.add(Dense(10, activation='softmax'))\n\n# Compiling the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nstart_time = time.time()\n# Training the model\nhistory = model.fit(X_train, y_train, epochs=30, batch_size=100, validation_split=0.2) \n\n# Predicting the labels for test data\ny_pred = model.predict(X_test)\n\n# Converting predicted probabilities to class labels\ny_pred_classes = np.argmax(y_pred, axis=1)\n\n# Converting one-hot encoded labels to class labels\ny_true_classes = np.argmax(y_test, axis=1)\nend_time = time.time()\n# Calculating confusion matrix\nconfusion_mtx = confusion_matrix(y_true_classes, y_pred_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(cfxmtx, annot=True, fmt=\"d\", xticklabels = Classes, yticklabels = Classes)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,axes = plt.subplots(1,2, figsize=(15,8))\nfig.suptitle(\"The model's evaluation \",fontsize=20)\naxes[0].plot(history.history['loss'])\naxes[0].plot(history.history['val_loss'])\naxes[0].set_title('Model Loss')\naxes[0].set_ylabel('Loss')\naxes[0].set_xlabel('Epoch')\naxes[0].legend(['Train','Test'])\n\naxes[1].plot(history.history['accuracy'])\naxes[1].plot(history.history['val_accuracy'])\naxes[1].set_title('Model Accuracy')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_xlabel('Epoch')\naxes[1].legend(['Train','Test'])\nplt.show()\nperformance_test = model.evaluate(X_test, y_test, batch_size=100)\npred = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test_classes, y_pred_classes))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on test data\ny_pred = model.predict(X_test)\n\n# Convert predictions from one-hot encoding to class labels\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_test_classes = np.argmax(y_test, axis=1)\n\n# Calculate evaluation metrics\naccuracy = accuracy_score(y_test_classes, y_pred_classes)\nprecision = precision_score(y_test_classes, y_pred_classes, average='macro')\nrecall = recall_score(y_test_classes, y_pred_classes, average='macro')\nf1 = f1_score(y_test_classes, y_pred_classes, average='macro')\n\n# Print results\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-Score:\", f1)\nprint(\"Convergence time\", end_time - start_time)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Classifier\n\nThe CIFAR-10 data has been split into training and test samples using cifar10.load_data() function. The training data is then fit with the random forest classifier and the predictions are done on the test set. The accuracy, precision and recall are calculated for this model as the evaluation metrics. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.datasets import cifar10\n\n# Load CIFAR-10 dataset\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n\n# Flatten the images\nX_train = X_train.reshape(X_train.shape[0], -1)\nX_test = X_test.reshape(X_test.shape[0], -1)\n\n# Split the data into train and test sets\nX_train, X_test = X_train.astype('float32') / 255.0, X_test.astype('float32') / 255.0\n\n# Initialize Random Forest classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nstart_time = time.time()\n# Train the model\nrf.fit(X_train, y_train)\n\n# Predict on test data\ny_pred = rf.predict(X_test)\nend_time = time.time()\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"Convergence Time:\", end_time - start_time)\n# Print classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Create a heatmap of the confusion matrix\nsns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\",xticklabels = Classes, yticklabels = Classes)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM With Kernel\n\nThis is a Kernel based SVM which uses the 'rbf' Kernel. First PCA is performed on the dataset for dimensionality reduction. This will improve the performance of SVM on the data. Then a pipeline is created with the PCA component and the SVM to which the data is fed. Once the data is fed into the pipeline the predictions are made","metadata":{}},{"cell_type":"code","source":"# Load CIFAR-10 dataset\nfrom keras.datasets import cifar10\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n\n# Flatten the images\nX_train = X_train.reshape(X_train.shape[0], -1)\nX_test = X_test.reshape(X_test.shape[0], -1)\n\n# Split the dataset into training and test sets\nX_train, X_val_svm, y_train, y_val_svm = train_test_split(X_train, y_train, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform PCA for dimensionality reduction\npca = PCA(n_components=100, random_state=42)\n\n# Create SVM classifier\nsvm = SVC(kernel='rbf', C=10, random_state=42)\n\n# Create a pipeline with PCA and SVM\npipeline = make_pipeline(StandardScaler(), pca, svm)\nstart_time = time.time()\n# Fit the pipeline to training data\npipeline.fit(X_train, y_train)\n\n# Predict on validation set\ny_pred_val_svm = pipeline.predict(X_val_svm)\n\n# Calculate accuracy on validation set\nval_accuracy = accuracy_score(y_val_svm, y_pred_val_svm)\nprint(f'Validation Accuracy: {val_accuracy:.2f}')\n\n\n# Predict on test set\ny_pred_test_svm = pipeline.predict(X_test)\nend_time = time.time()\n# Calculate accuracy on test set\ntest_accuracy = accuracy_score(y_test, y_pred_test_svm)\nprint(f'Test Accuracy: {test_accuracy:.2f}')\nprint('Convergence time', end_time - start_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncm = confusion_matrix(y_test, y_pred_test_svm)\n\n# Plot the confusion matrix\nplt.figure(figsize=(10, 7))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels = Classes, yticklabels = Classes)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred_test_svm))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}